{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution fitting to data\n",
    "\n",
    "SciPy has over 80 distributions that may be used to either generate data or test for fitting of exisitng data. In this example we will test for fit against ten distributions and plot the best three fits. For a full list of distributions see:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "In this example we'll take the first feature (column) from the Wisconsin Breast Cancer data set. \n",
    "\n",
    "As usual we will start by loading general modules used, and load our data (selecting the first column for our 'y', the data to be fitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load data and select first column\n",
    "\n",
    "from sklearn import datasets\n",
    "data_set = datasets.load_breast_cancer()\n",
    "y=data_set.data[:,0]\n",
    "\n",
    "# Create an index array (x) for data\n",
    "\n",
    "x = np.arange(len(y))\n",
    "size = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the data, and show descriptive summary\n",
    "\n",
    "Before we do any fitting of distributions, it's always good to do a simple visualisation of the data, and show desriptive statistics. We may, for example, decide to perform some outlier removal if we think that necessary (if there appear to be data points that don't belong to the rest of the population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put the data in a Pandas Dataframe we can use the Pandas secribe method to show a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y, columns=['Data'])\n",
    "y_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a range of distribution and test for goodness of fit\n",
    "\n",
    "This method will compare will fit a number of distributions to our data, compare goodness of fit with an approximate chi-squared, and test for significant difference between observed and fitted distribution with a Kolmogorov-Smirnov test.\n",
    "\n",
    "The approximate chi-squared bins data into 50 bins (this could be reduced for smaller data sets) based on percentiles so that each bin contains approximately an equal number of values. For each fitted distribution the expected count of values in each bin predicted from the distribution. The chi-squared value is the the sum of the relative squared error for each bin, such that:\n",
    "\n",
    "chi-squared = sum ((observed - predicted) ** 2) / predicted)\n",
    "\n",
    "For the observed and predicted we will use the cumulative sum of observed and predicted frequency across the bin range used.\n",
    "\n",
    "The lower the chi-squared value the better the fit.\n",
    "\n",
    "The Kolmogorov-Smirnov test assumes that data has been standardised: that is the mean is subtracted from all data (so the data becomes centred around zero), and that the results values are divided by the standard deviation (so all data becomes expressed as the number of standard deviations above or below the mean). A value of greater than 0.05 means that the fitted distribution is not significantly different to the observed distribution of the data.\n",
    "\n",
    "It is worth noting that statistical distributions are theoretical models of real-world data. Statistical distributions offer a good way of approximating data (and simplifying huge amounts of data into a few parameters). But when you have a large set of real-world data it is not surprising to find that no theoretical distribution fits the data perfectly.  Having he Kolmogorov-Smirnov tests for all distributions produce results of P<0.05 (fitted distribution is statistically different to the observed data distribution) is not unusual for large data sets. In that case in modelling we are generally happy to continue with a fit that looks 'reasonable', being aware this is one of the simplifications present in any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first standardise the data using sklearn's StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler() \n",
    "yy = y.reshape (-1,1)\n",
    "sc.fit(yy)\n",
    "y_std =sc.transform(yy)\n",
    "y_std = y_std.flatten()\n",
    "y_std\n",
    "del yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit 10 different distributions, rank them by the approximate chi-squared goodness of fit, and report the Kolmogorov-Smirnov (KS) P value results. Remember that we want chi-squared to be as low as possible, and ideally we want the KS P-value to be >0.05.\n",
    "\n",
    "Python may report warnings while running the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set list of distributions to test\n",
    "# See https://docs.scipy.org/doc/scipy/reference/stats.html for more\n",
    "\n",
    "# Turn off code warnings (this is not recommended for routine use)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up list of candidate distributions to use\n",
    "# See https://docs.scipy.org/doc/scipy/reference/stats.html for more\n",
    "\n",
    "dist_names = ['beta',\n",
    "              'expon',\n",
    "              'gamma',\n",
    "              'lognorm',\n",
    "              'norm',\n",
    "              'pearson3',\n",
    "              'triang',\n",
    "              'uniform',\n",
    "              'weibull_min', \n",
    "              'weibull_max']\n",
    "\n",
    "# Set up empty lists to stroe results\n",
    "chi_square = []\n",
    "p_values = []\n",
    "\n",
    "# Set up 50 bins for chi-square test\n",
    "# Observed data will be approximately evenly distrubuted aross all bins\n",
    "percentile_bins = np.linspace(0,100,101)\n",
    "percentile_cutoffs = np.percentile(y_std, percentile_bins)\n",
    "observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))\n",
    "cum_observed_frequency = np.cumsum(observed_frequency)\n",
    "\n",
    "# Loop through candidate distributions\n",
    "\n",
    "for distribution in dist_names:\n",
    "    # Set up distribution and get fitted distribution parameters\n",
    "    dist = getattr(scipy.stats, distribution)\n",
    "    param = dist.fit(y_std)\n",
    "    \n",
    "    # Obtain the KS test P statistic, round it to 5 decimal places\n",
    "    p = scipy.stats.kstest(y_std, distribution, args=param)[1]\n",
    "    p = np.around(p, 5)\n",
    "    p_values.append(p)    \n",
    "    \n",
    "    # Get expected counts in percentile bins\n",
    "    # This is based on a 'cumulative distrubution function' (cdf)\n",
    "    cdf_fitted = dist.cdf(percentile_cutoffs, *param[:-2], loc=param[-2], \n",
    "                          scale=param[-1])\n",
    "    expected_frequency = []\n",
    "    for bin in range(len(percentile_bins)-1):\n",
    "        expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]\n",
    "        expected_frequency.append(expected_cdf_area)\n",
    "    \n",
    "    # calculate chi-squared\n",
    "    expected_frequency = np.array(expected_frequency) * size\n",
    "    cum_expected_frequency = np.cumsum(expected_frequency)\n",
    "    ss = sum (((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency)\n",
    "    chi_square.append(ss)\n",
    "        \n",
    "# Collate results and sort by goodness of fit (best at top)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['Distribution'] = dist_names\n",
    "results['chi_square'] = chi_square\n",
    "results['p_value'] = p_values\n",
    "results.sort_values(['chi_square'], inplace=True)\n",
    "    \n",
    "# Report results\n",
    "\n",
    "print ('\\nDistributions sorted by goodness of fit:')\n",
    "print ('----------------------------------------')\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the top three fits, plot the fit and return the sklearn parameters. This time we will fit to the raw data rather than the standardised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide the observed data into 100 bins for plotting (this can be changed)\n",
    "number_of_bins = 100\n",
    "bin_cutoffs = np.linspace(np.percentile(y,0), np.percentile(y,99),number_of_bins)\n",
    "\n",
    "# Create the plot\n",
    "h = plt.hist(y, bins = bin_cutoffs, color='0.75')\n",
    "\n",
    "# Get the top three distributions from the previous phase\n",
    "number_distributions_to_plot = 3\n",
    "dist_names = results['Distribution'].iloc[0:number_distributions_to_plot]\n",
    "\n",
    "# Create an empty list to stroe fitted distribution parameters\n",
    "parameters = []\n",
    "\n",
    "# Loop through the distributions ot get line fit and paraemters\n",
    "\n",
    "for dist_name in dist_names:\n",
    "    # Set up distribution and store distribution paraemters\n",
    "    dist = getattr(scipy.stats, dist_name)\n",
    "    param = dist.fit(y)\n",
    "    parameters.append(param)\n",
    "    \n",
    "    # Get line for each distribution (and scale to match observed data)\n",
    "    pdf_fitted = dist.pdf(x, *param[:-2], loc=param[-2], scale=param[-1])\n",
    "    scale_pdf = np.trapz (h[0], h[1][:-1]) / np.trapz (pdf_fitted, x)\n",
    "    pdf_fitted *= scale_pdf\n",
    "    \n",
    "    # Add the line to the plot\n",
    "    plt.plot(pdf_fitted, label=dist_name)\n",
    "    \n",
    "    # Set the plot x axis to contain 99% of the data\n",
    "    # This can be removed, but sometimes outlier data makes the plot less clear\n",
    "    plt.xlim(0,np.percentile(y,99))\n",
    "\n",
    "# Add legend and display plot\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Store distribution paraemters in a dataframe (this could also be saved)\n",
    "dist_parameters = pd.DataFrame()\n",
    "dist_parameters['Distribution'] = (\n",
    "        results['Distribution'].iloc[0:number_distributions_to_plot])\n",
    "dist_parameters['Distribution parameters'] = parameters\n",
    "\n",
    "# Print parameter results\n",
    "print ('\\nDistribution parameters:')\n",
    "print ('------------------------')\n",
    "\n",
    "for index, row in dist_parameters.iterrows():\n",
    "    print ('\\nDistribution:', row[0])\n",
    "    print ('Parameters:', row[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
