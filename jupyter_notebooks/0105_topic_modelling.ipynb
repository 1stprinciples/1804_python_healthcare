{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling (dividing documents into topic groups) with Gensim\n",
    "\n",
    "Gensim is a library that can sort documents into groups. It is an 'unsupervised' method, meaning that documents do not need to be pre-labbeled. \n",
    "\n",
    "Here we will use gensim to group titles or keywords from PubMed scientific paper references.\n",
    "\n",
    "Gensim is not part of the standard Anaconda Python installation, but it may be installed from the command line with:\n",
    "\n",
    "conda install gensim \n",
    "\n",
    "If you are not using an Anaconda installation of Python then you can install with pip:\n",
    "\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Now we will load our data (the script below loads from a local copy of the imdb movie review database, but instructions are also given for downloading from the internet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use a portion of a large dataset of pubmed medical paper titles and keywords. The full data set may be downloaded from the link below (1.2GB download).\n",
    "\n",
    "https://www.kaggle.com/hsrobo/titlebased-semantic-subject-indexing#pubmed.csv\n",
    "\n",
    "The code section below downloads a 50k subset of the full data. It will download and save locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD 50k DATA SET FROM INTERNET\n",
    "\n",
    "file_location = 'https://gitlab.com/michaelallen1966/1804_python_healthcare_wordpress' + \\\n",
    "    '/raw/master/jupyter_notebooks/pubmed_50k.csv'\n",
    "data = pd.read_csv(file_location)\n",
    "# save to current directory\n",
    "data.to_csv('pubmed_50k.csv', index=False)\n",
    "\n",
    "\n",
    "# If you already have the data locally, you may load with:\n",
    "# data = pd.read_csv('pubmed_50k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We will clean data by applying the following steps.\n",
    "\n",
    "* convert all text to lower case\n",
    "* divide strings/sentences into individual words ('tokenize')\n",
    "* remove non-text words \n",
    "* remove 'stop words' (commonly occurring words that have little value in model)\n",
    "\n",
    "In the example here we will take the keywords (called #labels' for each paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define function to clean text\n",
    "def pre_process_text(X):\n",
    "    cleaned_X = []\n",
    "    for raw_text in X:\n",
    "            # Convert to lower case\n",
    "        text = raw_text.lower()\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Keep only words (removes punctuation + numbers)\n",
    "        token_words = [w for w in tokens if w.isalpha()]\n",
    "\n",
    "        # Remove stop words\n",
    "        meaningful_words = [w for w in token_words if not w in stops]\n",
    "        \n",
    "        cleaned_X.append(meaningful_words)\n",
    "    return cleaned_X\n",
    "\n",
    "\n",
    "# Clean text\n",
    "raw_text = list(data['labels'])\n",
    "processed_text = pre_process_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topic model\n",
    "\n",
    "The following will create our topic model. We will divide the references into 50 different topic areas. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_text)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_text]\n",
    "model = gensim.models.LdaModel(corpus=corpus, \n",
    "                               id2word=dictionary,\n",
    "                               num_topics=50,\n",
    "                               passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the first topic, we see that keywords are largely related to molecular biology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0.14238602, 'sequence'),\n",
      "  (0.07095096, 'molecular'),\n",
      "  (0.068985716, 'acid'),\n",
      "  (0.06632707, 'dna'),\n",
      "  (0.052034967, 'amino'),\n",
      "  (0.045958135, 'data'),\n",
      "  (0.03165856, 'proteins'),\n",
      "  (0.03090581, 'base'),\n",
      "  (0.02128076, 'rna'),\n",
      "  (0.018465547, 'genetic'),\n",
      "  (0.01786064, 'bacterial'),\n",
      "  (0.017836036, 'viral'),\n",
      "  (0.014751778, 'animals'),\n",
      "  (0.013531377, 'nucleic'),\n",
      "  (0.013407393, 'genes'),\n",
      "  (0.013192138, 'analysis'),\n",
      "  (0.012144415, 'humans'),\n",
      "  (0.011643986, 'cloning'),\n",
      "  (0.011388958, 'phylogeny'),\n",
      "  (0.011024448, 'protein')],\n",
      " -1.8900328727623419)\n"
     ]
    }
   ],
   "source": [
    "# Print the keywords for the first topic\n",
    "from pprint import pprint # makes the output easier to read\n",
    "pprint(top_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at another topic (topic 10) we see keywords that are associated with cardiac surgical procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0.056376483, 'outcome'),\n",
      "  (0.0558772, 'treatment'),\n",
      "  (0.05120605, 'humans'),\n",
      "  (0.03075589, 'surgical'),\n",
      "  (0.030704997, 'complications'),\n",
      "  (0.028222634, 'postoperative'),\n",
      "  (0.026755776, 'coronary'),\n",
      "  (0.02651835, 'tomography'),\n",
      "  (0.026010627, 'heart'),\n",
      "  (0.023422625, 'male'),\n",
      "  (0.022875749, 'computed'),\n",
      "  (0.021913974, 'studies'),\n",
      "  (0.02180667, 'procedures'),\n",
      "  (0.019811377, 'myocardial'),\n",
      "  (0.01757028, 'cardiac'),\n",
      "  (0.015987962, 'artery'),\n",
      "  (0.015796969, 'female'),\n",
      "  (0.013579166, 'prosthesis'),\n",
      "  (0.012545248, 'valve'),\n",
      "  (0.01180034, 'history')],\n",
      " -3.1752669709698886)\n"
     ]
    }
   ],
   "source": [
    "pprint(top_topics[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show topics present in each document\n",
    "\n",
    "Each document may contain one or more topics. The first paper is highlighted as containing topics 4, 8, 19 and 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.0967338), (8, 0.27275458), (19, 0.4578644), (40, 0.09598056)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[corpus[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
