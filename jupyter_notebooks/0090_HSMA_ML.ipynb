{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example uses a data set built into sklearn. It classifies biopsy samples from breast cancer patients as ether malignant (cancer) or benign (no cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data_set = datasets.load_breast_cancer()\n",
    "\n",
    "# Set up features (X), labels (y) and feature names\n",
    "X = data_set.data\n",
    "y = data_set.target\n",
    "feature_names = data_set.feature_names\n",
    "label_names = data_set.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show first record feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      " 7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      " 5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      " 2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      " 2.750e-01 8.902e-02]\n"
     ]
    }
   ],
   "source": [
    "print (X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print (label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first 25 lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print (y[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a binary outcome. There are just two possibilities: benign or malignant. The methods described below will also work with problems with more than two possible classifications, but we'll keep things relatively simple here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test sets\n",
    "\n",
    "Data will be split randomly with 75% of the data used to train the machine learning model, and 25% used to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "        X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data using standardisation\n",
    "\n",
    "We scale data so that all features share similar scales. \n",
    "\n",
    "The X data will be transformed by standardisation. To standardise we subtract the mean and divide by the standard deviation. All data (training + test) will be standardised using the mean and standard deviation of the training set.\n",
    "\n",
    "We will use a scaler from sklearn (but we could do this manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialise a new scaling object for normalising input data\n",
    "sc=StandardScaler() \n",
    "\n",
    "# Set up the scaler just on the training set\n",
    "sc.fit(X_train)\n",
    "\n",
    "# Apply the scaler to the training and test sets\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first row of the raw and scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "[1.288e+01 1.822e+01 8.445e+01 4.931e+02 1.218e-01 1.661e-01 4.825e-02\n",
      " 5.303e-02 1.709e-01 7.253e-02 4.426e-01 1.169e+00 3.176e+00 3.437e+01\n",
      " 5.273e-03 2.329e-02 1.405e-02 1.244e-02 1.816e-02 3.299e-03 1.505e+01\n",
      " 2.437e+01 9.931e+01 6.747e+02 1.456e-01 2.961e-01 1.246e-01 1.096e-01\n",
      " 2.582e-01 8.893e-02]\n",
      "\n",
      "Scaled data\n",
      "[-0.36737431 -0.24219862 -0.32226491 -0.4700164   1.83425665  1.21709729\n",
      " -0.52651885  0.09871505 -0.35972527  1.39174201  0.15619514 -0.05861438\n",
      "  0.17858324 -0.11883777 -0.57112622 -0.1052128  -0.61143715  0.10804629\n",
      " -0.24792232 -0.17943819 -0.26773858 -0.22328682 -0.25111534 -0.37512237\n",
      "  0.55131326  0.25668614 -0.74440738 -0.10266883 -0.50914117  0.25431606]\n"
     ]
    }
   ],
   "source": [
    "print ('Raw data:')\n",
    "print (X_train[0])\n",
    "print ()\n",
    "print ('Scaled data')\n",
    "print (X_train_std[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit logistic regression model\n",
    "\n",
    "Our first machien learning model is a logistic regression model.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ml = LogisticRegression(C=1000)\n",
    "ml.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thatâ€™s it! We can now use the model to predict malignant vs. benign classification of patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = ml.predict(X_train_std)\n",
    "y_pred_test = ml.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting training data = 0.9929577464788732\n",
      "Accuracy of prediciting test data = 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print ('Accuracy of prediciting training data =', accuracy_train)\n",
    "print ('Accuracy of prediciting test data =', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the accuracy of fitting training data is significantly higher than the test data. This is known as over-fitting. There are two potential problems with over-fitting:\n",
    "\n",
    "1) If we test accuracy on the same data used to train the model we report a spuriously high accuracy. Our model is not actually as good as we report.\n",
    "2) The model is too tightly built around our training data.\n",
    "\n",
    "The solution to the first problem is to always report accuracy based on predicting the values of a test data set that was not used to train the model.\n",
    "\n",
    "The solution to the second problem is to use 'regularisation'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation 'loosens' the fit to the training data. It effectively moves all predictions a little closer to the average for all values. \n",
    "\n",
    "In our logistic regression model, the regularisation parameter is C. A C value of 1,000 means there is very little regularisation. Try changing the values down by factors of 10, re-run code blocks 9, 10 and 11 and see what happens to the accuracy of the model. What value of C do you think is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examinining the probability of outcome, and changing the sensitivity of the model to predicting a positive\n",
    "\n",
    "There may be cases where either:\n",
    "\n",
    "1) We want to see the probability of a given classification, or\n",
    "\n",
    "2) We want to adjust the sensitivity of predicting a certain outcome (e.g. for health screening we may choose to accept more false positives in order to minimise the number fo false negatives).\n",
    "\n",
    "For linear regression we use the output 'predict_proba'. This may also be used in other machine learning models such as random forests and neural networks (but for support vector machines the output 'decision_function' is used in place of predict_proba.\n",
    "\n",
    "Let's look at it in action.\n",
    "\n",
    "For this section we'll refit the model with regularisation of C=0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label probabilities:\n",
      "[[9.99999977e-01 2.27009500e-08]\n",
      " [2.31989267e-01 7.68010733e-01]\n",
      " [5.99718849e-02 9.40028115e-01]\n",
      " [1.58608410e-02 9.84139159e-01]\n",
      " [1.87433182e-03 9.98125668e-01]]\n",
      "\n",
      "Predicted labels:\n",
      "[0 1 1 1 1]\n",
      "\n",
      "Actual labels:\n",
      "[0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# We'll start by retraining the model with C=0.1\n",
    "\n",
    "ml = LogisticRegression(C=0.1)\n",
    "ml.fit(X_train_std,y_train)\n",
    "y_pred_test = ml.predict(X_test_std)\n",
    "\n",
    "# Calculate the predicted probability of outcome 1:\n",
    "\n",
    "y_pred_probability = ml.predict_proba(X_test_std)\n",
    "\n",
    "# Print first 5 values and the predicted label:\n",
    "\n",
    "print ('Predicted label probabilities:')\n",
    "print (y_pred_probability[0:5])\n",
    "print ()\n",
    "print ('Predicted labels:')\n",
    "print (y_pred_test[0:5])\n",
    "print ()\n",
    "print ('Actual labels:')\n",
    "print (y_test[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate false positive and false negatives . In this data set being clear of chance has a label '1', and having cancer has a label '0'. A false positive, that is a sample is classed as cancer when is not actually cancer has a predicted test label of 0 and an actual label of 0. A false negative (predicted no cancer when cancer is actually present) has a predicted label of 1 and and actual label of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 2\n",
      "False negatives: 0\n"
     ]
    }
   ],
   "source": [
    "fp = np.sum((y_pred_test == 1) & (y_test == 0))\n",
    "fn = np.sum((y_pred_test == 0) & (y_test == 1))\n",
    "\n",
    "print ('False positives:', fp)\n",
    "print ('False negatives:', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we are more concerned about false negatives. Let's adjust the probability cut-off to change the threshold for classification as having no cancer (predicted label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.75\n",
    "\n",
    "# Now let's make a prediction based on that new cutoff.\n",
    "# Column 1 contains the probability of no cancer\n",
    "\n",
    "new_prediction = y_pred_probability[:,1] >= cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's calculate our false positives and negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 8\n",
      "False negatives: 1\n"
     ]
    }
   ],
   "source": [
    "fp = np.sum((new_prediction == 0) & (y_test == 1))\n",
    "fn = np.sum((new_prediction == 1) & (y_test == 0))\n",
    "\n",
    "print ('False positives:', fp)\n",
    "print ('False negatives:', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have eliminated false negatives, but at the cost of more false postives. Try adjusting the cuttoff value. What value do you think is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model weights (coefficients)\n",
    "\n",
    "We can obtain the model weights (coefficients) for each of the features. Values that are more strongly positive or negative are most important. A positive number means that this feature is linked to a classification label of 1. A negative number means that this feature is linked to a classification label of 0. \n",
    "We can obtain the weights by using the method .coef_ (be careful to add the trailing underscore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34349044 -0.33178015 -0.33844463 -0.36708645 -0.12187052  0.00765482\n",
      "  -0.42255581 -0.40341256 -0.07540728  0.20606778 -0.44130835  0.01567654\n",
      "  -0.33062184 -0.3657388  -0.0561784   0.19323076 -0.01636882 -0.01956193\n",
      "   0.06890254  0.24517315 -0.53861132 -0.52077952 -0.50924417 -0.51384138\n",
      "  -0.38831286 -0.11229711 -0.41778047 -0.43760388 -0.3899574  -0.09911312]]\n"
     ]
    }
   ],
   "source": [
    "print (ml.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "A second type of categorisation model is a Random Forest.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Random_forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ml = RandomForestClassifier(n_estimators = 10000,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "# For random forests we don't need to use scaled data\n",
    "ml.fit (X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting test data = 0.9440559440559441\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels\n",
    "\n",
    "y_pred_test = ml.predict(X_test)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "print ('Accuracy of prediciting test data =', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Feature importances give us the relative importance of each feature - the higher the number the greater the influence on the decision (they add up to 1.0). Feature importances do not tell use which decision is more likely.\n",
    "\n",
    "(Careful to add the trailing underscore in ml.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature  importance\n",
      "22          worst perimeter    0.145793\n",
      "20             worst radius    0.126224\n",
      "23               worst area    0.120934\n",
      "27     worst concave points    0.107566\n",
      "7       mean concave points    0.086282\n",
      "6            mean concavity    0.050089\n",
      "2            mean perimeter    0.049308\n",
      "3                 mean area    0.046792\n",
      "0               mean radius    0.041615\n",
      "26          worst concavity    0.036971\n",
      "13               area error    0.035772\n",
      "25        worst compactness    0.016429\n",
      "21            worst texture    0.015007\n",
      "10             radius error    0.014587\n",
      "12          perimeter error    0.012707\n",
      "1              mean texture    0.012507\n",
      "28           worst symmetry    0.011385\n",
      "24         worst smoothness    0.010911\n",
      "5          mean compactness    0.010349\n",
      "29  worst fractal dimension    0.006442\n",
      "4           mean smoothness    0.005170\n",
      "11            texture error    0.005052\n",
      "16          concavity error    0.004991\n",
      "17     concave points error    0.004303\n",
      "15        compactness error    0.004159\n",
      "19  fractal dimension error    0.004147\n",
      "18           symmetry error    0.003951\n",
      "14         smoothness error    0.003708\n",
      "8             mean symmetry    0.003454\n",
      "9    mean fractal dimension    0.003395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['feature'] = feature_names\n",
    "df['importance'] = ml.feature_importances_\n",
    "df = df.sort_values('importance', ascending = False)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDITIONAL MATERIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "Support Vector Machines are another common classification algorithm.\n",
    "Regularisation (C) may be adjusted, and different 'kernels' may also be applied. The two most common are 'linear' and 'rbf')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting training data = 0.9953051643192489\n",
      "Accuracy of prediciting test data = 0.972027972027972\n",
      "\n",
      "Predicted label probabilities:\n",
      "[-0.7905539   6.09369495 16.11186543  6.55771422 21.89224697]\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "data_set = datasets.load_breast_cancer()\n",
    "\n",
    "# Set up features (X), labels (y) and feature names\n",
    "\n",
    "X = data_set.data\n",
    "y = data_set.target\n",
    "feature_names = data_set.feature_names\n",
    "label_names = data_set.target_names\n",
    "\n",
    "# Split data into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "        X,y,test_size=0.25)\n",
    "\n",
    "# Scale data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "# Note: a common test is to see whether linear or rbf kernel is best\n",
    "# Try changing regularisation (C)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "ml = SVC(kernel='linear',C=100)\n",
    "ml.fit(X_train_std,y_train)\n",
    "\n",
    "# Predict training and test set labels\n",
    "\n",
    "y_pred_train = ml.predict(X_train_std)\n",
    "y_pred_test = ml.predict(X_test_std)\n",
    "\n",
    "# Check accuracy of model\n",
    "\n",
    "import numpy as np\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "print ('Accuracy of prediciting training data =', accuracy_train)\n",
    "print ('Accuracy of prediciting test data =', accuracy_test)\n",
    "\n",
    "# Show classification probabilities for first five samples\n",
    "# Note that for SVMs we use decision_function, in place of predict_proba \n",
    "\n",
    "y_pred_probability = ml.decision_function(X_test_std)\n",
    "print ()\n",
    "print ('Predicted label probabilities:')\n",
    "print (y_pred_probability[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks may be better for very large or complex data sets. A challenge is the number of parameters that need to be optimised.\n",
    "\n",
    "After importing the MLPClassifier (another name for a Neural Network is a Mulit-Level Perceptron Classifier) type help (MLPClassifier) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting training data = 1.0\n",
      "Accuracy of prediciting test data = 0.972027972027972\n",
      "\n",
      "Predicted label probabilities:\n",
      "[[0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 2.25849723e-60]\n",
      " [1.00000000e+00 1.40199390e-70]\n",
      " [3.77475828e-15 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "data_set = datasets.load_breast_cancer()\n",
    "\n",
    "# Set up features (X), labels (y) and feature names\n",
    "\n",
    "X = data_set.data\n",
    "y = data_set.target\n",
    "feature_names = data_set.feature_names\n",
    "label_names = data_set.target_names\n",
    "\n",
    "# Split data into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "# Scale data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std=sc.transform(X_train)\n",
    "X_test_std=sc.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "# Note: a common test is to see whether linear or rbf kernel is best\n",
    "# Try changing regularisation (C)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "ml = MLPClassifier(solver='lbfgs',\n",
    "                   alpha=1e-8, \n",
    "                   hidden_layer_sizes=(50, 10),\n",
    "                   max_iter=100000, \n",
    "                   shuffle=True, \n",
    "                   learning_rate_init=0.001,\n",
    "                   activation='relu', \n",
    "                   learning_rate='constant', \n",
    "                   tol=1e-7,\n",
    "                   random_state=0)\n",
    "\n",
    "ml.fit(X_train_std, y_train) \n",
    "\n",
    "# Predict training and test set labels\n",
    "\n",
    "y_pred_train = ml.predict(X_train_std)\n",
    "y_pred_test = ml.predict(X_test_std)\n",
    "\n",
    "# Check accuracy of model\n",
    "\n",
    "import numpy as np\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "print ('Accuracy of prediciting training data =', accuracy_train)\n",
    "print ('Accuracy of prediciting test data =', accuracy_test)\n",
    "\n",
    "# Show classification probabilities for first five samples\n",
    "# Neural networks may often produce spuriously high proabilities!\n",
    "\n",
    "y_pred_probability = ml.predict_proba(X_test_std)\n",
    "print ()\n",
    "print ('Predicted label probabilities:')\n",
    "print (y_pred_probability[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Random Forest example with multiple categories\n",
    "\n",
    "We will use another classic 'toy' data set to look at multiple label classification. This is the categorisation of iris plants. We only have four features but have three different classification possibilities.\n",
    "We will use logistic regression, but all the methods described above work on multiple label classification. \n",
    "Note: for completeness of code we'll import the required modules again, but this is not actually necessary if they have been imported previously.\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names:\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Feature names:\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "First sample feature values:\n",
      "[5.1 3.5 1.4 0.2]\n",
      "\n",
      "Labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data_set = datasets.load_iris()\n",
    "\n",
    "X = data_set.data\n",
    "y = data_set.target\n",
    "feature_names = data_set.feature_names\n",
    "label_names = data_set.target_names\n",
    "\n",
    "print ('Label names:')\n",
    "print (label_names)\n",
    "print ()\n",
    "print ('Feature names:')\n",
    "print (feature_names)\n",
    "print ()\n",
    "print ('First sample feature values:')\n",
    "print (X[0])\n",
    "print ()\n",
    "print ('Labels:')\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data\n",
    "\n",
    "Here we'll use a random forest model which does not need scaling. But for all other model types a scaling step would be added here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit random forest model and show accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = RandomForestClassifier(n_estimators = 10000,\n",
    "                            random_state = 0,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "# For random forests we don't need to use scaled data\n",
    "ml.fit (X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict training and test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = ml.predict(X_train)\n",
    "y_pred_test = ml.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediciting training data = 1.0\n",
      "Accuracy of prediciting test data = 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print ('Accuracy of prediciting training data =', accuracy_train)\n",
    "print ('Accuracy of prediciting test data =', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show classification of first 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label:\n",
      "[2 0 2 1 2 2 0 0 1 1]\n",
      "\n",
      "Predicted label:\n",
      "[1 0 2 1 2 2 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print ('Actual label:')\n",
    "print (y_test[0:10])\n",
    "print ()\n",
    "print ('Predicted label:')\n",
    "print (y_pred_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing prediction probabilities and changing sensitivity to classification\n",
    "\n",
    "As with a binary classification we may be interested in obtaining the probability of label classification, either to get an indicator of the certainty of classification, or to bias classification towards or against particular classes.\n",
    "\n",
    "Changing sensitivity towards particular class labels is more complicated with multi-class problems, but the principle is the same as with a binary classification. We can access the calculated probabilities of classification for each label. Usually the one with the highest probability  is taken, but rules could be defined to bias decisions more towards one class if that is beneficial.\n",
    "\n",
    "Here we will just look at the probability outcomes for each class. The usual rule for prediction is to simply take the one that is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 8.058e-01 1.942e-01]\n",
      " [9.997e-01 1.000e-04 2.000e-04]\n",
      " [0.000e+00 1.100e-03 9.989e-01]\n",
      " [1.000e-04 9.988e-01 1.100e-03]\n",
      " [0.000e+00 2.200e-03 9.978e-01]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probability = ml.predict_proba(X_test)\n",
    "\n",
    "print (y_pred_probability[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
