{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 104: Using free text for classification – ‘Bag of Words’\n",
    "\n",
    "\n",
    "There may be times in healthcare where we would like to classify patients based on free text data we have for them. Maybe, for example, we would like to predict likely outcome based on free text clinical notes.\n",
    "\n",
    "Using free text requires methods known as ‘Natural Language Processing’.\n",
    "\n",
    "Here we start with one of the simplest techniques – ‘bag of words’.\n",
    "\n",
    "In a ‘bag of words’ free text is reduced to a vector (a series of numbers) that represent the number of times a word is used in the text we are given. It is also posible to look at series of two, three or more words in case use of two or more words together helps to classify a patient.\n",
    "\n",
    "A classic ‘toy problem’ used to help teach or develop methos is to try to judge whether people rates a film as ‘like’ or ‘did not like’ based on the free text they entered into a widely used internet film review database (www.imdb.com).\n",
    "\n",
    "Here will will use 50,000 records from IMDb to convert each review into a ‘bag of words’, which we will then use in a simple logistic regression machine learning model.\n",
    "\n",
    "We can use raw word counts, but in this case we’ll add an extra transformation called tf-idf (frequency–inverse document frequency) which adjusts values according to the number fo reviews that use the word. Words that occur across many reviews may be less discriminatory than words that occur more rarely, so tf-idf reduces the value of those words used frequently across reviews.\n",
    "\n",
    "This code will take us through the following steps:\n",
    "\n",
    "1) Load data from internet\n",
    "\n",
    "2) Clean data – remove non-text, convert to lower case, reduce words to their ‘stems’ (see below for details), and reduce common ‘stop-words’ (such as ‘as’, ‘the’, ‘of’).\n",
    "\n",
    "3) Split data into training and test data sets\n",
    "\n",
    "4) Convert cleaned reviews in word vectors (‘bag of words’), and apply the tf-idf transform.\n",
    "\n",
    "5) Train a logistic regression model on the tr-idf transformed word vectors.\n",
    "\n",
    "6) Apply the logistic regression model to our previously unseen test cases, and calculate accuracy of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If you do not already have the data locally you may download (and save) by\n",
    "\n",
    "file_location = 'https://gitlab.com/michaelallen1966/00_python_snippets' +\\\n",
    "    '_and_recipes/raw/master/machine_learning/data/IMDb.csv'\n",
    "imdb = pd.read_csv(file_location)\n",
    "# save to current directory\n",
    "imdb.to_csv('imdb.csv', index=False)\n",
    "\n",
    "# If you already have the data locally then you may run the following\n",
    "\n",
    "# Load data example\n",
    "imdb = pd.read_csv('imdb.csv')\n",
    "\n",
    "# Truncate data for example if you want to speed up the example\n",
    "# imdb = imdb.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to preprocess data\n",
    "\n",
    "This function, as previously described, works on raw text strings, and:\n",
    "\n",
    "1) changes to lower case\n",
    "2) tokenizes (breaks down into words\n",
    "3) removes punctuation and non-word text\n",
    "4) finds word stems\n",
    "5) removes stop words\n",
    "6) rejoins meaningful stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# If not previously performed:\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stemming = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def apply_cleaning_function_to_list(X):\n",
    "    cleaned_X = []\n",
    "    for element in X:\n",
    "        cleaned_X.append(clean_text(element))\n",
    "    return cleaned_X\n",
    "\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    \"\"\"This function works on a raw text string, and:\n",
    "        1) changes to lower case\n",
    "        2) tokenizes (breaks down into words\n",
    "        3) removes punctuation and non-word text\n",
    "        4) finds word stems\n",
    "        5) removes stop words\n",
    "        6) rejoins meaningful stem words\"\"\"\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = raw_text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Keep only words (removes punctuation + numbers)\n",
    "    # use .isalnum to keep also numbers\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_words = [stemming.stem(w) for w in token_words]\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in stemmed_words if not w in stops]\n",
    "    \n",
    "    # Rejoin meaningful stemmed words\n",
    "    joined_words = ( \" \".join(meaningful_words))\n",
    "    \n",
    "    # Return cleaned data\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the data cleaning function (this may take a few minutes if you are using the full 50,000 reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text to clean\n",
    "text_to_clean = list(imdb['review'])\n",
    "\n",
    "# Clean text\n",
    "cleaned_text = apply_cleaning_function_to_list(text_to_clean)\n",
    "\n",
    "# Add cleaned data back into DataFrame\n",
    "imdb['cleaned_review'] = cleaned_text\n",
    "\n",
    "# Remove temporary cleaned_text list (after transfer to DataFrame)\n",
    "del cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = list(imdb['cleaned_review'])\n",
    "y = list(imdb['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 'bag of words'\n",
    "\n",
    "The 'bag of words' is the word vector for each review. This may be a simple word count for each review where each position of the vector represnts a word (returned in the 'represents' list) and the value of that position represents the number of times that word is used in the review. \n",
    "\n",
    "The function below also returns a tf-idf (frequency–inverse document frequency) which adjusts values according to the number fo reviews that use the word. Words that occur across many reviews may be less discriminatory than words that occur more rarely. The tf-idf transform reduces the value of a given word in proportion to the number of documents that it appears in. \n",
    "\n",
    "The function returns the following:\n",
    "\n",
    "1) vectorizer - this may be applied to any new reviews to convert the revier into the same word vector as the training set.\n",
    "\n",
    "2) vocab - the list of words that the word vectors refer to.\n",
    "\n",
    "3) train_data_features - raw word count vectors for each review\n",
    "\n",
    "4) tfidf_features - tf-idf transformed word vectors\n",
    "\n",
    "5) tfidf - the tf-idf transformation that may be applied to new reviews to convert the raw word counts into the transformed word counts in the same way as the training data.\n",
    "\n",
    "Our vectorizer has an argument called 'ngram_range'. A simple bag of words divides reviews into single words. If we have an ngram_range of (1,2) it means that the review is divided into single words and also pairs of consecutive words. This may be useful if pairs of words are useful, such as 'very good'. The max_features argument limits the size of the word vector, in this case to a maximum of 10,000 words (or 10,000 ngrams of words if an ngram may be more than one word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    print ('Creating bag of words...')\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.  \n",
    "    \n",
    "    # In this example features may be single words or two consecutive words\n",
    "    # (as shown by ngram_range = 1,2)\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000\n",
    "                                ) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings. The output is a sparse array\n",
    "    train_data_features = vectorizer.fit_transform(X)\n",
    "    \n",
    "    # Convert to a NumPy array for easy of handling\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    # tfidf transform\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
    "\n",
    "    # Get words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "   \n",
    "    return vectorizer, vocab, train_data_features, tfidf_features, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our bag of words function to our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bag of words...\n"
     ]
    }
   ],
   "source": [
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf  = \\\n",
    "    create_bag_of_words(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a DataFrame of our words and counts, so that we may sort and view them. The count and tfidf_features exist for each X (each review in this case) - here we will look at just the first review (index 0).\n",
    "\n",
    "Note that the tfidf_features differ from the count; that is because of the adjustment for how commonly they occur across reviews. \n",
    "\n",
    "(Try changing the sort to sort by tfidf_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ngram  count  tfidf_features\n",
      "9320        wa      4        0.139373\n",
      "5528      movi      3        0.105926\n",
      "9728     whole      2        0.160024\n",
      "3473    german      2        0.249079\n",
      "6327      part      2        0.140005\n",
      "293   american      1        0.089644\n",
      "9409   wa kind      1        0.160155\n",
      "9576      wast      1        0.087894\n",
      "7380       saw      1        0.078477\n",
      "7599      sens      1        0.085879\n"
     ]
    }
   ],
   "source": [
    "bag_dictionary = pd.DataFrame()\n",
    "bag_dictionary['ngram'] = vocab\n",
    "bag_dictionary['count'] = train_data_features[0]\n",
    "bag_dictionary['tfidf_features'] = tfidf_features[0]\n",
    "\n",
    "# Sort by raw count\n",
    "bag_dictionary.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "# Show top 10\n",
    "print(bag_dictionary.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training a machine learning model on the bag of words\n",
    "\n",
    "Now we have transformed our free text reviews in vectors of numbers (representing words) we can apply many different machine learning techniques. Here will will use a relatively simple one, logistic regression.\n",
    "\n",
    "We’ll set up a function to train a logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(features, label):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ml_model = LogisticRegression(C = 100,random_state = 0)\n",
    "    ml_model.fit(features, label)\n",
    "    print ('Finished')\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the tf-idf tranformed word vectors to train the model (we could use the plain word counts contained in ‘train_data_features’ (rather than using ’tfidf_features’). We pass both the features and the known label corresponding to the review (the sentiment, either 0 or 1 depending on whether a person likes the film or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the logistic regression model...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "ml_model = train_logistic_regression(tfidf_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the bag of words model to test reviews\n",
    "\n",
    "We will now apply the bag of words model to test reviews, and assess the accuracy.\n",
    "\n",
    "We’ll first apply our vectorizer to create a word vector for review in the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(X_test)\n",
    "# Convert to numpy array\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using the tf-idf transform, we’ll apply the tfid transformer so that word vectors are transformed in the same way as the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "# Convert to numpy array\n",
    "test_data_tfidf_features = test_data_tfidf_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the bit that we really want to do – we’ll predict the sentiment of the all test reviews (and it’s just a single line of code!). Did they like the film or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 87%\n"
     ]
    }
   ],
   "source": [
    "predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "correctly_identified_y = predicted_y == y_test\n",
    "accuracy = np.mean(correctly_identified_y) * 100\n",
    "print ('Accuracy = %.0f%%' %accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
