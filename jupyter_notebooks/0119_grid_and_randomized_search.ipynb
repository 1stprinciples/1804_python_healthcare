{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising scikit-learn machine learning models with grid search or randomized search\n",
    "\n",
    "Machine learning models have many hyper-parameters (parameters set before a model is fitted, and which remain constant throughout model fitting). Optimising model hyper-parameters may involve many model runs with alternative hyper-parameters. In SciKit-Learn, this may be performed in an automated fashion using Grid Search (which explores all combinations of provided hyper-parameters) or Randomized Search (which randomly selects combinations to test). \n",
    "\n",
    "Grid search and randomized search will perform this optimisation using k-fold validation which avoids potential bias in training/test splits.\n",
    "\n",
    "Here we will revisit a previous example of machine learning, using Random Forests to predict whether a person has breast cancer. We will then use Grid Search to optimise performance, using the 'f1' performance score (https://en.wikipedia.org/wiki/F1_score) as an accuracy score that balances the importance of false negatives and false positives.\n",
    "\n",
    "First we will look at how we previously built the Random Forests model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine learning diagnostic performance measures:\n",
      "-------------------------------------------------\n",
      "accuracy = 0.951\n",
      "sensitivity = 0.956\n",
      "specificity = 0.943\n",
      "positive_likelihood = 16.881\n",
      "negative_likelihood = 0.047\n",
      "false_positive_rate = 0.057\n",
      "false_negative_rate = 0.044\n",
      "positive_predictive_value = 0.966\n",
      "negative_predictive_value = 0.926\n",
      "precision = 0.966\n",
      "recall = 0.956\n",
      "f1 = 0.961\n",
      "positive_rate = 0.622\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_diagnostic_performance (actual_predicted):\n",
    "    \"\"\" Calculate diagnostic performance.\n",
    "    \n",
    "    Takes a Numpy array of 1 and zero, two columns: actual and predicted\n",
    "    \n",
    "    Note that some statistics are repeats with different names\n",
    "    (precision = positive_predictive_value and recall = sensitivity).\n",
    "    Both names are returned\n",
    "    \n",
    "    Returns a dictionary of results:\n",
    "        \n",
    "    1) accuracy: proportion of test results that are correct    \n",
    "    2) sensitivity: proportion of true +ve identified\n",
    "    3) specificity: proportion of true -ve identified\n",
    "    4) positive likelihood: increased probability of true +ve if test +ve\n",
    "    5) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    6) false positive rate: proportion of false +ves in true -ve patients\n",
    "    7) false negative rate:  proportion of false -ves in true +ve patients\n",
    "    8) positive predictive value: chance of true +ve if test +ve\n",
    "    9) negative predictive value: chance of true -ve if test -ve\n",
    "    10) precision = positive predictive value \n",
    "    11) recall = sensitivity\n",
    "    12) f1 = (2 * precision * recall) / (precision + recall)\n",
    "    13) positive rate = rate of true +ve (not strictly a performance measure)\n",
    "    \"\"\"\n",
    "    # Calculate results\n",
    "    actual_positives = actual_predicted[:, 0] == 1\n",
    "    actual_negatives = actual_predicted[:, 0] == 0\n",
    "    test_positives = actual_predicted[:, 1] == 1\n",
    "    test_negatives = actual_predicted[:, 1] == 0\n",
    "    test_correct = actual_predicted[:, 0] == actual_predicted[:, 1]\n",
    "    accuracy = np.average(test_correct)\n",
    "    true_positives = actual_positives & test_positives\n",
    "    true_negatives = actual_negatives & test_negatives\n",
    "    sensitivity = np.sum(true_positives) / np.sum(actual_positives)\n",
    "    specificity = np.sum(true_negatives) / np.sum(actual_negatives)\n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    false_positive_rate = 1 - specificity\n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    positive_predictive_value = np.sum(true_positives) / np.sum(test_positives)\n",
    "    negative_predictive_value = np.sum(true_negatives) / np.sum(test_negatives)\n",
    "    precision = positive_predictive_value\n",
    "    recall = sensitivity\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    positive_rate = np.mean(actual_predicted[:,1])\n",
    "    \n",
    "    # Add results to dictionary\n",
    "    performance = {}\n",
    "    performance['accuracy'] = accuracy\n",
    "    performance['sensitivity'] = sensitivity\n",
    "    performance['specificity'] = specificity\n",
    "    performance['positive_likelihood'] = positive_likelihood\n",
    "    performance['negative_likelihood'] = negative_likelihood\n",
    "    performance['false_positive_rate'] = false_positive_rate\n",
    "    performance['false_negative_rate'] = false_negative_rate\n",
    "    performance['positive_predictive_value'] = positive_predictive_value\n",
    "    performance['negative_predictive_value'] = negative_predictive_value\n",
    "    performance['precision'] = precision\n",
    "    performance['recall'] = recall\n",
    "    performance['f1'] = f1\n",
    "    performance['positive_rate'] = positive_rate\n",
    "\n",
    "    return performance\n",
    "\n",
    "def load_data ():\n",
    "    \"\"\"Load the data set. Here we load the Breast Cancer Wisconsin (Diagnostic)\n",
    "    Data Set. Data could be loaded from other sources though the structure\n",
    "    should be compatible with thi sdata set, that is an object with the \n",
    "    following attribtes:\n",
    "        .data (holds feature data)\n",
    "        .feature_names (holds feature titles)\n",
    "        .target_names (holds outcome classification names)\n",
    "        .target (holds classification as zero-based number)\n",
    "        .DESCR (holds text-based description of data set)\"\"\"\n",
    "    \n",
    "    data_set = datasets.load_breast_cancer()\n",
    "    return data_set\n",
    "\n",
    "def normalise (X_train,X_test):\n",
    "    \"\"\"Normalise X data, so that training set has mean of zero and standard\n",
    "    deviation of one\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc=StandardScaler() \n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "    # Apply the scaler to the training and test sets\n",
    "    X_train_std=sc.transform(X_train)\n",
    "    X_test_std=sc.transform(X_test)\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "\n",
    "def print_diagnostic_results (performance):\n",
    "    \"\"\"Iterate through, and print, the performance metrics dictionary\"\"\"\n",
    "    \n",
    "    print('\\nMachine learning diagnostic performance measures:')\n",
    "    print('-------------------------------------------------')\n",
    "    for key, value in performance.items():\n",
    "        print (key,'= %0.3f' %value) # print 3 decimal places\n",
    "    return\n",
    "\n",
    "def print_feaure_importances (model, features):\n",
    "    print ()\n",
    "    print ('Feature importances:')\n",
    "    print ('--------------------')\n",
    "    df = pd.DataFrame()\n",
    "    df['feature'] = features\n",
    "    df['importance'] = model.feature_importances_\n",
    "    df = df.sort_values('importance', ascending = False)\n",
    "    print (df)\n",
    "    return\n",
    "\n",
    "def split_data (data_set, split=0.25):\n",
    "    \"\"\"Extract X and y data from data_set object, and split into tarining and\n",
    "    test data. Split defaults to 75% training, 25% test if not other value \n",
    "    passed to function\"\"\"\n",
    "    \n",
    "    X=data_set.data\n",
    "    y=data_set.target\n",
    "    X_train,X_test,y_train,y_test=train_test_split(\n",
    "        X,y,test_size=split, random_state=0)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def test_model(model, X, y):\n",
    "    \"\"\"Return predicted y given X (attributes)\"\"\"\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    test_results = np.vstack((y, y_pred)).T\n",
    "    return test_results\n",
    "\n",
    "def train_model (X, y):\n",
    "    \"\"\"Train the model. Note n_jobs=-1 uses all cores on a computer\"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    model.fit (X,y)\n",
    "    return model\n",
    "\n",
    "###### Main code #######\n",
    "\n",
    "# Load data\n",
    "data_set = load_data()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train,X_test,y_train,y_test = split_data(data_set, 0.25)\n",
    "\n",
    "# Normalise data (not needed for Random Forests)\n",
    "# X_train_std, X_test_std = normalise(X_train,X_test)\n",
    "\n",
    "# Train model\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# Produce results for test set\n",
    "test_results = test_model(model, X_test, y_test)\n",
    "\n",
    "# Measure performance of test set predictions\n",
    "performance = calculate_diagnostic_performance(test_results)\n",
    "\n",
    "# Print performance metrics\n",
    "print_diagnostic_results(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise with grid search\n",
    "\n",
    "NOTE: Grid search may take considerable time to run!\n",
    "\n",
    "Grid search enables us to perform an exhaustive search of hyper-parameters (those model parameters that are constant in any one model). We define which hyper-parameters we wish to change, and what values we wish to try. All combinations are tested. Test are performed using k-fold validation which re-runs the model with different train/test splits (this avoids bias in our train/test split, but does increase the time required). You may wish to time small grid search first, so you have a better idea of how many parameter combinations you can realistically look at.\n",
    "\n",
    "\n",
    "We pass four arguments to the grid search method:\n",
    "\n",
    "1) The range of values for the hyper-parameters, defined in a dictionary\n",
    "2) The machine learning model to use\n",
    "3) The number of k-fold splits to use (`cv`); a value of 5 will give five 80:20 training/test splits with each sample being present in the test set once\n",
    "4) The accuracy score to use. In a classification model `‘accuracy’` is common. For a regression model using `scoring='neg_mean_squared_error'` is common (for grid search an accuracy score must be a 'utility function' rather than a 'cost function', that is, higher values are better). \n",
    "\n",
    "If the best model uses a value at one extreme of the provided hyper-paramter ranges then it is best to expand the range of that hyper-paraemter to be sure an optimum has been found.\n",
    "\n",
    "More info on grid search: https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "An alternative approach is randomised hyper-parameter searching. See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Use Grid search to optimise\n",
    "# n_jobs is set to -1 to use all cores on the CPU\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [10, 30, 100, 300, 1000, 3000],\n",
    "              'bootstrap': [True, False],\n",
    "              'min_samples_split': [2, 4, 6, 8, 10],\n",
    "              'n_jobs': [-1]}\n",
    "\n",
    "# Grid search will use k-fold cross-validation (CV is number of splits)\n",
    "# Grid search also needs a ultility function (higher is better) rather than\n",
    "# a cost function (lower is better) so use neg square mean error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_grid = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(forest_grid, param_grid, cv=10,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train); #';' suppresses printed output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimised model hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'min_samples_split': 4, 'n_estimators': 30, 'n_jobs': -1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show best parameters\n",
    "# If best parameters are at the extremes of the searches then extend the range\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, show full description\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the optimised model. We could use the text above (from the output of `grid_search.best_estimator_`, or we can use `grid_search.best_estimator_` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimised model\n",
    "model = grid_search.best_estimator_\n",
    "model.fit (X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test optimised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine learning diagnostic performance measures:\n",
      "-------------------------------------------------\n",
      "accuracy = 0.972\n",
      "sensitivity = 0.978\n",
      "specificity = 0.962\n",
      "positive_likelihood = 25.911\n",
      "negative_likelihood = 0.023\n",
      "false_positive_rate = 0.038\n",
      "false_negative_rate = 0.022\n",
      "positive_predictive_value = 0.978\n",
      "negative_predictive_value = 0.962\n",
      "precision = 0.978\n",
      "recall = 0.978\n",
      "f1 = 0.978\n",
      "positive_rate = 0.629\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(model, X_test, y_test)\n",
    "\n",
    "# Measure performance of test set predictions\n",
    "performance = calculate_diagnostic_performance(test_results)\n",
    "\n",
    "# Print performance metrics\n",
    "print_diagnostic_results(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has now increased from 95.1% to 97.2%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise with randomized search\n",
    "\n",
    "When the number of parameter combinations because unreasonable large for grid search, and alternative is to use random search, which will select parameters randomly from the ranges given. The number of combinations tried is given by the argument `n_iter`.\n",
    "\n",
    "Below is an example where we expand the number of arguments varied (becoming too large for grid search) and use random search to test 50 different samples.\n",
    "\n",
    "For more details see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Use Grid search to optimise\n",
    "# n_jobs is set to -1 to use all cores on the CPU\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [10, 30, 100, 300, 1000, 3000],\n",
    "              'bootstrap': [True, False],\n",
    "              'min_samples_split': range(2,11),\n",
    "              'max_depth': range(1,30),\n",
    "              'min_samples_split': [2, 4, 6, 8, 10],\n",
    "              'n_jobs': [-1]}\n",
    "\n",
    "n_iter_search = 50\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_grid = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(forest_grid, param_grid, cv=10,\n",
    "                           n_iter=n_iter_search, scoring='accuracy')\n",
    "\n",
    "random_search.fit(X_train, y_train); #';' suppresses printed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_jobs': -1,\n",
       " 'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'max_depth': 29,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show best parameters\n",
    "# If best parameters are at the extremes of the searches then extend the range\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=29, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or, show full description\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimised model\n",
    "model = random_search.best_estimator_\n",
    "model.fit (X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine learning diagnostic performance measures:\n",
      "-------------------------------------------------\n",
      "accuracy = 0.986\n",
      "sensitivity = 0.989\n",
      "specificity = 0.981\n",
      "positive_likelihood = 52.411\n",
      "negative_likelihood = 0.011\n",
      "false_positive_rate = 0.019\n",
      "false_negative_rate = 0.011\n",
      "positive_predictive_value = 0.989\n",
      "negative_predictive_value = 0.981\n",
      "precision = 0.989\n",
      "recall = 0.989\n",
      "f1 = 0.989\n",
      "positive_rate = 0.629\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(model, X_test, y_test)\n",
    "\n",
    "# Measure performance of test set predictions\n",
    "performance = calculate_diagnostic_performance(test_results)\n",
    "\n",
    "# Print performance metrics\n",
    "print_diagnostic_results(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
