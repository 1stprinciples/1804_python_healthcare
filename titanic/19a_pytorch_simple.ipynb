{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - PyTorch 'sequential' neural net.\n",
    "\n",
    "In this workbook we build a neural network to predict survival. The two common frameworks used for neural networks (as of 2020) are TensorFlow and PyTorch. Both are excellent frameworks, but PyTorch is more natively Python in its syntax, and also allows for easier debugging (as the model may be interrupted, with a breakpoint, and debugged as necessary).\n",
    "\n",
    "Both TensorFlow and PyTorch allow the neural network to be trained on a GPU, which is beneficial for large neural networks (especially those processing image or sound data). \n",
    "\n",
    "Installation instructions for PyTorch may be found at pytorch.org. (If in doubt about what installation to use, use `pip install` and use CPU-only, not CUDA).\n",
    "\n",
    "There are two versions of this workbook. This version uses a simpler form of constructing the neural network. The alternative version uses a class-based method which offers some more flexibility (but at the cost of a little simplicity). It is recommended to work through both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn for pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of acuuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted neagtive rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true postive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predicitive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_positives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predicitive_value'] = negative_predicitive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to scalte data\n",
    "\n",
    "In neural networks it is common to to scale input data 0-1 rather than use standardisation (subtracting mean and dividing by standard deviation) of each feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural net\n",
    "\n",
    "Here we use the `sequential` method to set up a PyTorch neural network. This simpler method assumes each layer occurs in sequence. Though simpler, it lacks some flexibility, and does not allow for easy debugging by setting a breakpoint in the middle of the training sequence.\n",
    "\n",
    "We will put construction of the neural net into a separate function.\n",
    "\n",
    "The neural net is a simple two layer network, but contains some useful additions (batch normalisation and dropout) as decribed below.\n",
    "\n",
    "The layers of the network are:\n",
    "\n",
    "1) An input layer (which does not need to be defined) \n",
    "2) A linear fully-connected (dense) layer.This is defined by the number of inputs (the number of input features) and the number of outputs. We will expand out feature data set up to 240 outputs.\n",
    "3) A batch normalisation layer. This is not usually used for small models, but can increase the speed of training for larger models. It is added here as an example of how to include it (in large models all dense layers would be followed by a batch normalisation layer). The layer definition includes the number of inputs to normalise.\n",
    "4) A dropout layer. This layer randomly sets outputs from the preceding layer to zero during training (a different set of outputs is zeroed for each training iteration). This helps prevent over-fitting of the model to the training data. Typically between 0.1 and 0.3 outputs are set to zero (`p=0.1` means 10% of outputs are set to zero).\n",
    "5) An activation layer. In this case ReLU (rectified linear unit). ReLU activation is most common for the inner layers of a neural network. Negative input values are set to zero. Positive input values are left unchanged.\n",
    "6) A layer to reduce output of ReLU down to two linear nodes\n",
    "7) Apply sigmid activation to convert each node to range 0-1 output.\n",
    "\n",
    "The output of the net are two numbers (corrsponding to scored for died/survived). These do not necessarily add up to one. The one with the highest value will be taken as the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net(number_features):\n",
    "\n",
    "    net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(number_features, 240),\n",
    "            torch.nn.BatchNorm1d(240),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.ReLU(240),\n",
    "            torch.nn.Linear(240,2),            \n",
    "            torch.nn.Sigmoid())\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_fold 1 0.8100558659217877\n",
      "K_fold 2 0.8033707865168539\n",
      "K_fold 3 0.8146067415730337\n",
      "K_fold 4 0.7921348314606742\n",
      "K_fold 5 0.848314606741573\n"
     ]
    }
   ],
   "source": [
    "# Set up lists to hold results\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "\n",
    "# Set up splits\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "k_counter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_np, y_np):\n",
    "    k_counter +=1\n",
    "    print('K_fold {}'.format(k_counter),end=' ')\n",
    "    \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "    \n",
    "    # Scale X data\n",
    "    X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "    \n",
    "    # Define network\n",
    "    number_features = X_train_sc.shape[1]\n",
    "        \n",
    "    net = make_net(number_features)\n",
    "    \n",
    "    ### Train model\n",
    "    # Note: Lots of these parameters may be fine tuned\n",
    "    \n",
    "    # Set batch size (cases per batch - commonly 8-64)\n",
    "    batch_size = 16\n",
    "    # Epochs (number of times to pass over data)\n",
    "    num_epochs = 200\n",
    "    # Learning rate (how much each bacth updates the model)\n",
    "    learning_rate = 0.003\n",
    "    # Calculate numebr of batches\n",
    "    batch_no = len(X_train_sc) // batch_size\n",
    "    \n",
    "    # Set up optimizer for classification\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train model by passing through the data the required number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(batch_no):\n",
    "            # Get X and y batch data\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_var = Variable(torch.FloatTensor(X_train_sc[start:end]))\n",
    "            y_var = Variable(torch.LongTensor(y_train[start:end]))\n",
    "            # These steps train the model: Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            ypred_var = net(x_var)\n",
    "            loss = criterion(ypred_var, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    ### Test model (print results for each k-fold iteration)\n",
    "    \n",
    "    test_var = Variable(torch.FloatTensor(X_train_sc))\n",
    "    result = net(test_var)\n",
    "    values, labels = torch.max(result, 1)\n",
    "    y_pred_train = labels.data.numpy()\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    training_acc_results.append(accuracy_train)\n",
    "    \n",
    "    test_var = Variable(torch.FloatTensor(X_test_sc))\n",
    "    result = net(test_var)\n",
    "    values, labels = torch.max(result, 1)\n",
    "    y_pred_test = labels.data.numpy()\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    print(accuracy_test)\n",
    "    test_acc_results.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8665730337078652,\n",
       " 0.8751753155680224,\n",
       " 0.8513323983169705,\n",
       " 0.8401122019635343,\n",
       " 0.8387096774193549]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on training data\n",
    "training_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8100558659217877,\n",
       " 0.8033707865168539,\n",
       " 0.8146067415730337,\n",
       " 0.7921348314606742,\n",
       " 0.848314606741573]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on test data\n",
    "test_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854, 0.814\n"
     ]
    }
   ],
   "source": [
    "# Get mean results\n",
    "mean_training = np.mean(training_acc_results)\n",
    "mean_test = np.mean(test_acc_results)\n",
    "\n",
    "# Display each to three decimal places\n",
    "print ('{0:.3f}, {1:.3}'.format(mean_training,mean_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results: Box Plot\n",
    "\n",
    "Box plots show median (orange line), the secons and third quartiles (the box), the range (excluding outliers), and any outliers as 'whisker' points. Outliers, by convention, are conisdered to be any points outside of the quartiles +/- 1.5 times the interquartile range. The limit for outliers may be changed using the optional `whis` argument in the boxplot.\n",
    "\n",
    "Medians tend to be an easy reliable guide to the centre of a distribution (i.e. look at the medians to see whether a fit is improving or not, but also look at the box plot to see how much variability there is).\n",
    "\n",
    "Test sets tend to be more variable in their accuracy measures. Can you think why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up X data \n",
    "x_for_box = [training_acc_results, test_acc_results]\n",
    "\n",
    "# Set up X labels\n",
    "labels = ['Training', 'Test'] \n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "# Add subplot (can be used to define multiple plots in same figure)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Define Box Plot (`widths` is optional)\n",
    "ax1.boxplot(x_for_box, \n",
    "            widths=0.7)\n",
    "\n",
    "# Set X and Y labels\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
