{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic survival - K-fold stratification\n",
    "\n",
    "In our previous example* using logistic regression to classify passengers as likely to survive the Titanic, we used a random split for training and test data. But doing a single assessment like this may lead to an inaccurate assesment of the accuracy.\n",
    "\n",
    "We could use repeated random splits, but a more robust method is to use ‘stratified k-fold validation’. In this method the model is repeated k times, so that all the data is used once, but only once, as part of the test set. This, alone, is k-fold validation. Stratified k-fold validation adds an extra level of robustness by ensuring that in each of the k training/test splits, the balance of outcomes represents the balance of outcomes (between survivors and non-survivors)in the overall data set. Most commonly 5 or 10 different splits of the data are used.\n",
    "\n",
    "In a full project it is common to also have some hold-back test data that is used only at the end of model development (with k-fold validation used during model development).\n",
    "\n",
    "*https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/02_logistic_regression.ipynb\n",
    "\n",
    "*In this notebook we assume that you have run through the basic logistic regression example in the previous example. We will not explain all steps fully*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Run the following code if data for Titanic survival has not been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The loading of data assumes that data has been downloaded and saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (lables)\n",
    "\n",
    "We will split into features (X) and label (y) and convert from a Pamdas DataFrame to NumPy arrays. NumPy arrays are simpler to refer to by row/column index numbers, and sklearn's k-fold method provides row indices for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into two DataFrames\n",
    "X_df = data.drop('Survived',axis=1)\n",
    "y_df = data['Survived']\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to standardise data\n",
    "Standardisation subtracts the mean and divides by the standard deviation, for each feature.\n",
    "Here we use the sklean built-in method for standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Converts all data to a smiliar scale.\n",
    "    Standardisation subtracts mean and divides by standard deviation\n",
    "    for each feature.\n",
    "    Standardised data will have a mena of 0 and standard deviation of 1.\n",
    "    The training data mean and standard deviation is used to standardise both\n",
    "    training and test set data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the model for all k-fold splits\n",
    "\n",
    "The following code:\n",
    "* Sets up lists to hold results for each k-fold split\n",
    "* Sets up the k-fold splits using sklearn's `StratifiedKFold` method\n",
    "* Trains a logistic regression model, and test its it, for eack k-fold split\n",
    "* Adds each k-fold training/test accuracy to the lists\n",
    "\n",
    "Here wer are using a simple accuracy score, the proportion of predicts that are correct. K-fold validation may also be used for more complicated accuracy assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lists to hold results for each k-fold run\n",
    "training_acc_results = []\n",
    "test_acc_results = []\n",
    "\n",
    "# Set up splits\n",
    "number_of_splits = 10\n",
    "skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Standardise X data\n",
    "    X_train_std, X_test_std = standardise_data(X_train, X_test)\n",
    "    \n",
    "    # Set up and fit model\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train_std,y_train)\n",
    "    \n",
    "    # Predict training and test set labels\n",
    "    y_pred_train = model.predict(X_train_std)\n",
    "    y_pred_test = model.predict(X_test_std)\n",
    "    \n",
    "    # Calculate accuracy of training and test sets\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    # Add accuracy to lists\n",
    "    training_acc_results.append(accuracy_train)\n",
    "    test_acc_results.append(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training and test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8139825218476904,\n",
       " 0.8102372034956304,\n",
       " 0.816708229426434,\n",
       " 0.8017456359102244,\n",
       " 0.816708229426434,\n",
       " 0.8154613466334164,\n",
       " 0.814214463840399,\n",
       " 0.8117206982543641,\n",
       " 0.8104738154613467,\n",
       " 0.8144458281444583]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on training data\n",
    "training_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7666666666666667,\n",
       " 0.7888888888888889,\n",
       " 0.7752808988764045,\n",
       " 0.8651685393258427,\n",
       " 0.7640449438202247,\n",
       " 0.7640449438202247,\n",
       " 0.7865168539325843,\n",
       " 0.797752808988764,\n",
       " 0.8202247191011236,\n",
       " 0.8068181818181818]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show individual accuracies on test data\n",
    "test_acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813, 0.794\n"
     ]
    }
   ],
   "source": [
    "# Get mean results\n",
    "mean_training = np.mean(training_acc_results)\n",
    "mean_test = np.mean(test_acc_results)\n",
    "\n",
    "# Display each to three decimal places\n",
    "print ('{0:.3f}, {1:.3f}'.format(mean_training,mean_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the average accuracy is better for the training than the test sets. This is due to the model being slightly 'over-fitted' to the training data, a topic we shall return to in a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results: Box Plot\n",
    "\n",
    "Box plots show median (orange line), the secons and third quartiles (the box), the range (excluding outliers), and any outliers as 'whisker' points. Outliers, by convention, are conisdered to be any points outside of the quartiles +/- 1.5 times the interquartile range. The limit for outliers may be changed using the optional `whis` argument in the boxplot.\n",
    "\n",
    "Medians tend to be an easy reliable guide to the centre of a distribution (i.e. look at the medians to see whether a fit is improving or not, but also look at the box plot to see how much variability there is).\n",
    "\n",
    "Test sets tend to be more variable in their accuracy measures. Can you think why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up X data \n",
    "x_for_box = [training_acc_results, test_acc_results]\n",
    "\n",
    "# Set up X labels\n",
    "labels = ['Training', 'Test'] \n",
    "\n",
    "# Set up figure\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "# Add subplot (can be used to define multiple plots in same figure)\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "# Define Box Plot (`widths` is optional)\n",
    "ax1.boxplot(x_for_box, \n",
    "            widths=0.7,\n",
    "            whis=10)\n",
    "\n",
    "# Set X and Y labels\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
